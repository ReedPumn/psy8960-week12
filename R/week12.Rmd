---
title: "PSY 8960: Week 12 Project"
author: "Reed Priest"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
---

## Script Settings and Resources

```{r Set wd and load packages}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(RedditExtractoR)
library(tm)
library(qdap)
library(textstem)
library(RWeka)
library(doParallel)
library(ldatuning)
library(topicmodels)
library(tidytext)
library(wordcloud)
```

## Data Import and Cleaning
```{r Data import}
# The last approach I used trying to scrape Reddit data using fromJSON produced the same issues from before with page scrolling. So after some Chat GPTing, I found the RedditExtractoR package. It downloaded all the data I needed using this code, but it took about 15 minutes to do so. This package isn't part of the course materials, but it was an effective solution. See this page for details: https://github.com/ivan-rivera/RedditExtractor.

 #reddit_urls <- find_thread_urls(subreddit = "IOPsychology", sort_by = "new", period = "year")
# reddit_content <- get_thread_content(reddit_urls$url)
# week12_tbl <- as_tibble(reddit_content$threads) %>%
  # select(title = title, upvotes = upvotes)
# write_csv(week12_tbl, file = "../data/week12data.csv")

# By calling the previously downloaded data, I avoid the need to download it again.
week12_tbl <- read_csv(file = "../data/week12data.csv")
```

```{r Corpus setup}
# This is our base corpus we will use to contain all of the Reddit documents over the past year.
io_corpus_original <- VCorpus(VectorSource(week12_tbl$title))
# This trimmed corpus is similar to the original, but removes unnecessary information. I did not use many custom stop words given the relatively small number of documents we are analyzing (about 1000).
io_corpus_trimmed <- io_corpus_original %>%
  tm_map(content_transformer(replace_abbreviation)) %>%
  tm_map(content_transformer(replace_contraction)) %>%
  tm_map(content_transformer(str_to_lower)) %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, c(stopwords("en"), "io psychology")) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(lemmatize_words)) %>%
  # Even after removing io-relevant stopwords, empty cases still exist. So we remove them with this function
  tm_filter(FUN = function(x) {
    return(nchar(stripWhitespace(x$content)[[1]]) > 1)
  })
```

```{r Comparing corpuses}
# This function compares our two corpuses to let us check whether the trimmed corpus properly trimmed unnecessary information.
compare_them <- function(corpus_origingal, corpus_trimmed) {
  possible <- 1:length(io_corpus_original)
  chosen <- sample(possible, 1)
  compare <- list("Original" = content(io_corpus_original[[chosen]]), "Comparison" = content(io_corpus_trimmed[[chosen]]))
  return(compare)
}
# With this replicate function, I can easily compare a decent number of entries at once.
replicate(20, compare_them())
```

```{r Create DTMs}
# This function uses a max value of 2 to represent our inclusion of bigrams. This is useful for some words like "human resources".
tokenizer <- function(x) {
  NGramTokenizer(x, Weka_control(min = 1, max = 2))
}
# Now we apply our function to the trimmed corpus.
io_dtm <- DocumentTermMatrix(io_corpus_trimmed, control = list(tokenize = tokenizer))
# All sparse terms are eliminated with this sparse argument value.
io_slim_dtm <- removeSparseTerms(io_dtm, .997)
# These lines of code calculate our n:k ratio. I played with the numbers a bit, and setting the sparse argument in the above line to .997 is the first value that reached our desired n:k threshold.
n <- length(io_corpus_trimmed)
k <- ncol(io_slim_dtm)
n/k
```


## Analysis
```{r Categorizing posgts}
# I am using paralellized processing to speed up this part of analyses. If you are running these analyses on a computer with less than 8 cores, you may want to not run this code.
num_clusters <- makeCluster(7)
registerDoParallel(num_clusters)
# This line finds the number of topics in our data. I chose to compare findings across all four models to triangulate results.
lda_tuning <- FindTopicsNumber(io_dtm, topics = seq(5, 20, by = 1), metrics = c( "Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"), verbose = TRUE)
# This line plots the previous possible numbers of topics.
FindTopicsNumber_plot(lda_tuning)
stopCluster(num_clusters)
registerDoSEQ()
# It seems like 10 topics exist in our data.
```

```{r Analyzing categories}
# I chose to analyze our data with 10 topics based on findings from the previous plots.
lda_results <- LDA(io_dtm, 10)
# This line documents our beta matrix, noting the likelihood of each word appearing in the data.
lda_betas <- tidy(lda_results, matrix = "beta")
# Similarly, this line documents our gamma matrix, noting the likelihood fo each document appearing in the data.
lda_gammas <- tidy(lda_results, matrix = "gamma")
# This series of pipes produces the top words in each category. It's very helpful to see what topics each category focuses on.
betas <- lda_betas %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(topic, -beta)

# This series of pipes does a similar procedure for our gamma values with documents.
gammas <- lda_gammas %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  slice(1) %>%
  ungroup %>%
  mutate(document = as.numeric(document)) %>%
  arrange(document)

# This series of lines creates our week12_tbl. We first create two tibbles with ids to enable joining.
week12_tbl_with_ids <- week12_tbl %>%
  mutate(doc_id = as.character(1:nrow(week12_tbl)))
topics_tbl_with_ids <- tibble(doc_id = Docs(io_dtm))
# We then create the tibble with the four columns we desire.
week12_tbl <- topics_tbl_with_ids %>%
  left_join(y = week12_tbl_with_ids) %>%
  mutate(original = title,
         topic = gammas$topic,
         probability = gammas$gamma) %>%
  select(-title, -upvotes)
```

```{r Creating final tibble}
# This final tibble adds the upvotes column to the week12_tbl.
final_tbl <- week12_tbl %>%
  full_join(week12_tbl_with_ids) %>%
  select(-title)

# Now conduct a statistical test to see if upvotes significantly differs by topic. We are comparing a DV across multiple IVs, so we run an ANOVA.
summary(aov(upvotes ~ topic, data = final_tbl))
# There was not a statistically significant difference between the number of upvotes across our topics. As a result, post-hoc analyses are not to be conducted.
```

```{r Interpretations}
# Q1: Using the beta matrix alone, what topics would you conclude your final topic list maps onto? (e.g., topic 1, 2, 3â€¦n each reflect what substantive topic construct? Use your best judgment.)
# A1: From the previous topic plots, I chose 10 topics. I labeled these topics as follows: 1) career searching, 2) disseminating content, 3) professional development, 4) career development, 5) I-O functions, 6) seeking advice, 7) training programs, 8) general I-O psychology, 9) analysis, and 10) graduate school.

# Q2: Look at the original text of documents with the highest and lowest probabilities assigned to each document. Do your topic names derived from your interpretation of the beta matrix conceptually match with the content of the original posts? What kind of validity evidence does your answer to this question represent?
# A2: No, each topic does not have a perfectly clean set of indices in which the highest probabilities clearly represent the given topic. At best, about half of the documents within a given topic "hang around" the same title for that topic. As a result, an argument could be made to choose less topics. The process of exploring possible themes in our data resembles factorial validity, particularly through exploratory factor analysis.
```

## Visualization
```{r Wordcloud}
# These two lines of code create our word cloud. The specific layout of the word cloud changes upon each calculation, but the basic message is the same.
wordcloud <- as_tibble(as.matrix(io_dtm))
wordcloud(names(wordcloud), colSums(wordcloud), max.words = 10)
# Interpreting the size of the words indicates their commonality in the data. It seems like "research", "work", and "job" are common words due to their large size. This makes sense considering how I-O psychologists research people's jobs and work.
```